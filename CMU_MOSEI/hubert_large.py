# -*- coding: utf-8 -*-
"""「HubertForSST-2.ipynb」的副本

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SNV-fgQMCOzevsS7W4uuUTRx--0nbznW

### Some testing ...
"""

# data = pd.read_csv('glue_data/SST-2/train.tsv', sep='\t')
# print(type(data['label'].loc[3]))





"""### Import Packages"""

import os
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import torchvision.transforms as transforms
from transformers import AdamW, Wav2Vec2Processor, Wav2Vec2ForCTC
from transformers import HubertForSequenceClassification, Wav2Vec2FeatureExtractor, HubertModel, HubertConfig
from transformers import pipeline
from torch.utils.data import Dataset, DataLoader, Subset
from torchvision.datasets import DatasetFolder
from tqdm.auto import tqdm
import librosa
import random

"""### Dataset"""

import random
class MyDataset(Dataset):
  def __init__(self, mode):
    self.mode = mode
    self.fold = None
    if self.mode == "train":
        from folds import standard_train_fold
        self.fold = standard_train_fold
    elif self.mode == "valid":
        from folds import standard_valid_fold
        self.fold = standard_valid_fold
    elif self.mode == "test":
        from folds import standard_test_fold
        self.fold = standard_test_fold
    self.labels_path = './Raw_b/Labels/labels.csv'
    self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("facebook/hubert-large-ls960-ft")
    self.labels = pd.read_csv(self.labels_path)
    self.labels = self.labels[self.labels["video_id"].isin(self.fold).values].reset_index(drop=True).drop("Unnamed: 0", axis=1) 
    print(f"Finish loading {self.mode} data with length {len(self.labels)}")

  def __getitem__(self, index):
    id = self.labels["video_id"][index]
    speech, _ = librosa.load(f"./Raw_b/Audio/Full/WAV_16000/{id}.wav", sr=16000, mono=True)
    import math
    inputs = self.feature_extractor(speech[max(0, math.floor(self.labels["interval_start"][index]*16000)):math.ceil(self.labels["interval_end"][index]*16000)], sampling_rate=16000, padding="max_length", max_length=50000, return_attention_mask=True, return_tensors="pt")
    mask = inputs.attention_mask
    inputs = inputs.input_values
    max_length = 500000
    if inputs.shape[1] > max_length:
        r = random.randint(0, inputs.shape[1]-max_length)
        inputs = inputs[0][r:r + max_length].unsqueeze(0)
        mask = mask[0][r:r + max_length].unsqueeze(0)
        #print(self.labels["video_id"][index], inputs.shape)
    # print(math.floor(self.labels["interval_start"][index]*16000), math.ceil(self.labels["interval_end"][index]*16000), inputs.shape)
    return inputs, mask, self.labels["sentiment"][index] + 3


  def __len__(self):
    return len(self.labels)

"""### Define model"""

# model = HubertForSequenceClassification.from_pretrained("superb/hubert-base-superb-ks", output_hidden_states = True, output_attentions=False)
# params = list(model.named_parameters())
# print('The BERT model has {:} different named parameters.\n'.format(len(params)))

class Classifier(nn.Module):
  def __init__(self):
    super(Classifier, self).__init__()
    configuration = HubertConfig(num_labels = 7, use_weighted_layer_sum=True, classifier_proj_size=768)
    self.hubert_layers = HubertForSequenceClassification(configuration)
    self.hubert_layers.hubert.from_pretrained("facebook/hubert-large-ls960-ft")
    self.hubert_layers.freeze_feature_extractor()
    self.hubert_layers.freeze_base_model()
    #self.hubert_layers.freeze_base_model()
    #self.hubert_layers = HubertModel.from_pretrained("superb/hubert-base-superb-ks")

  def forward(self, x, mask):
    x = self.hubert_layers(input_values = x, attention_mask = mask)
    #print(f'x = {x}')
    return x.logits

"""### Training"""

train_dataset = MyDataset('train')
valid_dataset = MyDataset('valid')
test_dataset = MyDataset("test")
train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, drop_last=False)
valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, drop_last=False)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, drop_last=False)

###
device = "cuda:1" if torch.cuda.is_available() else "cpu"
print(f"running on {device}")

model = Classifier().to(device)
model.to(device)
try:
    model.load_state_dict(torch.load("hubert_l.ckpt", map_location=device))
except:
    pass

# Freeze the first 6 layers

criterion = nn.CrossEntropyLoss()

optimizer = AdamW(model.parameters(), lr=0.00003, betas=(0.9, 0.98), weight_decay=0)

n_epochs = 15
accu_step = 1
best_acc = 0
for epoch in range(n_epochs):
  model.train()

  train_loss = []
  train_accs = []
  step = 0
  for batch in tqdm(train_loader):
    wavs, mask, labels = batch
    wavs = torch.squeeze(wavs, 1).to(device)
    mask = mask.to(device)
    #print(wavs.shape)
    logits = model(wavs, mask)

    loss = criterion(logits, labels.to(device))
    train_loss.append(loss.item())
    loss /= accu_step
    loss.backward()
    step += 1
    if step % accu_step == 0:
        optimizer.step()
        optimizer.zero_grad()
    if step == 12000:
        for name, param in model.hubert_layers.hubert.named_parameters():
            if f"layers" in name:
                    param.requires_grad = True

    acc = (logits.argmax(dim=-1).cpu() == labels.cpu()).float().mean()

    train_accs.append(acc)
    if(step % 100 == 0):
        print(f"Loss: {sum(train_loss) / len(train_loss)}")
  train_loss = sum(train_loss) / len(train_loss)
  train_acc = sum(train_accs) / len(train_accs)

  print(f"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}")

  model.eval()
  valid_loss = []
  valid_accs = []

  for batch in tqdm(valid_loader):
    wavs, mask, labels = batch
    wavs = torch.squeeze(wavs, 0)
    with torch.no_grad():
      logits = model(wavs.to(device), mask.to(device))
    loss = criterion(logits, labels.to(device))
    acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()
    valid_loss.append(loss.item())
    valid_accs.append(acc)
  valid_loss = sum(valid_loss) / len(valid_loss)
  valid_acc = sum(valid_accs) / len(valid_accs)
  if valid_acc >= best_acc:
      best_acc = valid_acc
      print(f"Save model with acc {best_acc}")
      torch.save(model.state_dict(), "hubert_l.ckpt")

  print(f"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}")
