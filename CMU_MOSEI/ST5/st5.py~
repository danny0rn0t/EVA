"""### Import Packages"""

import os
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import torchvision.transforms as transforms
from transformers import AdamW, Wav2Vec2Processor
from transformers import HubertForSequenceClassification, Wav2Vec2FeatureExtractor, HubertModel, HubertConfig
from transformers import T5TokenizerFast, T5Model
from transformers import pipeline
from torch.utils.data import Dataset, DataLoader, Subset
from torchvision.datasets import DatasetFolder
from tqdm.auto import tqdm
import librosa
import random
import os
import sys
from objective import NTXent

config = {
        "train_batch_size": 8,
        "logging_step": 1000,
        "padding_length": 250000,
        "max_length": 250000,
        "sample_rate": 16000,
        "lr":0.003,
        "max_text_length": 200,
        "text_model": "t5-base",
        "audio_model": "facebook/hubert-base-ls960"
        }
"""### Dataset"""

import random
import soundfile as sf
import bisect
import sox
class MyDataset(Dataset):
  def __init__(self, mode):
    self.mode = mode
    self.fold = None
    self.file_root = "../../Librispeech/LibriSpeech/"
    if self.mode == "train":
        self.file_path=["train-clean-100", "train-clean-360", "train-other-500"]
    elif self.mode == "valid":
        self.file_path = ["dev-clean", "dev-other"]
    elif self.mode == "test":
        pass

    self.dataPath = []
    self.prefixSum = []
    self.length = 0
    for d in self.file_path:
        for sub_d in os.listdir(self.file_root + d):
            for sub_sub_d in os.listdir(self.file_root + d + "/" + sub_d):
                item_num = len(os.listdir(self.file_root + d + "/" + sub_d + "/" + sub_sub_d)) - 1
                self.dataPath.append(d + "/" + sub_d + "/" + sub_sub_d)
                self.prefixSum.append(self.length)
                self.length += item_num

    self.tokenizer = T5TokenizerFast.from_pretrained(config["text_model"])
    self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(config["audio_model"])
    self.transformer = sox.Transformer()
    self.transformer.trim()
    self.transformer.compand()
    print(self.transformer.effects_log)
    print(f"Finish loading {self.mode} data with length {self.length}")

  def __getitem__(self, index):
    idx = bisect.bisect_right(self.prefixSum, index)
    offset = index - self.prefixSum[idx - 1]
    prefix = "-".join(self.dataPath[idx - 1].split("/")[-2:])
    with open(self.file_root + self.dataPath[idx - 1] + "/" + prefix + ".trans.txt", "r") as fp:
        lines = fp.readlines()
        for line in lines:
            if prefix in line:
                text = " ".join(line.split(" ")[1:]).lower().strip()
                break
    if(not len(text)):
        print("Error")
        exit()
    speech, sr = sf.read(self.file_root + self.dataPath[idx - 1] + "/" + prefix + "-{:04}.flac".format(offset))
    assert sr == config["sample_rate"]
    speech = self.transformer.build_array(input_array=speech, sample_rate_in=config["sample_rate"])
    '''
    if len(self.tokenizer(text, add_special_tokens = False)["input_ids"]) > config["max_text_length"]:
        print(f"Exceed text length {len(self.tokenizer(text, add_special_tokens = False)['input_ids'])}")
    if len(speech) > config["max_length"]:
        print("Exceed audio length")
    '''
    a_inputs = self.feature_extractor(speech, sampling_rate=config["sample_rate"], padding="max_length", max_length=config["padding_length"], truncation=True,return_attention_mask=True, return_tensors="pt")
    t_inputs = self.tokenizer(text, add_special_tokens=False, truncation=True, max_length=config["max_text_length"], return_tensors="pt", padding="max_length")

    return a_inputs["input_values"], a_inputs["attention_mask"], t_inputs["input_ids"], t_inputs["attention_mask"]


  def __len__(self):
    return self.length

"""### Define model"""

# model = HubertForSequenceClassification.from_pretrained("superb/hubert-base-superb-ks", output_hidden_states = True, output_attentions=False)
# params = list(model.named_parameters())
# print('The BERT model has {:} different named parameters.\n'.format(len(params)))

# transformer or conformer?
class PoolingHead(nn.Module):
    def __init__(self):
        super(PoolingHead, self).__init__()
        encoder_layer = nn.TransformerEncoderLayer(d_model=768, nhead=8)
        self.pooling = nn.TransformerEncoder(encoder_layer, num_layers=2)

    def forward(self, x):
        return self.pooling(x)[:, 0, :]

class AcousticEncoder(nn.Module):
  def __init__(self):
    super(AcousticEncoder, self).__init__()
    self.hubert_layers = HubertModel.from_pretrained("facebook/hubert-base-ls960")
    #self.hubert_layers.feature_extractor._freeze_parameters()

  def forward(self, x, mask):
    x = self.hubert_layers(input_values = x, attention_mask = mask)
    #print(f'x = {x}')
    return x

class DualEncoder(nn.Module):
    def __init__(self):
        super(DualEncoder, self).__init__()
        self.textModel = T5Model.from_pretrained(config["text_model"])
        self.acousticModel = AcousticEncoder()
        self.PoolingHead = PoolingHead()
        for name, param in self.textModel.named_parameters():
            param.requires_grad = False

    def forward(self, audio, audio_mask, text, text_mask):
        text_encoding = self.textModel.encoder(text, attention_mask = text_mask)
        text_encoding = self.PoolingHead(text_encoding.last_hidden_state)
        audio_encoding = self.acousticModel(audio, audio_mask)
        audio_encoding = self.PoolingHead(audio_encoding.last_hidden_state)
        return text_encoding, audio_encoding

"""### Training"""

train_dataset = MyDataset('train')
valid_dataset = MyDataset('valid')
#test_dataset = MyDataset("test")
train_loader = DataLoader(train_dataset, batch_size=config["train_batch_size"], shuffle=True, drop_last=False)
valid_loader = DataLoader(valid_dataset, batch_size=config["train_batch_size"], shuffle=False, drop_last=False)
#test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, drop_last=False)

###
device = "cuda:0" if torch.cuda.is_available() else "cpu"
print(f"running on {device}")

model = DualEncoder().to(device)
model.to(device)
optimizer = AdamW(model.parameters(), lr=config["lr"], betas=(0.9, 0.98))
try:
    checkpoint = torch.load("st5.ckpt", map_location=device)
    model.load_state_dict(checkpoint["model"])
    optimizer.load_state_dict(checkpoint["optimizer"])
    print("Successfully load model")
except:
    pass

criterion = NTXent()

n_epochs = 150
accu_step = 1
best_acc = 0
for epoch in range(n_epochs):
  model.train()

  train_loss = []
  train_accs = []
  step = 0
  for batch in tqdm(train_loader, file=sys.stdout):
    wavs, wav_masks, texts, text_masks = batch
    wavs = torch.squeeze(wavs, 1).to(device)
    wav_masks = torch.squeeze(wav_masks, 1).to(device)
    texts = torch.squeeze(texts, 1).to(device)
    text_masks = torch.squeeze(text_masks, 1).to(device)
    wav_embed, text_embed = model(wavs, wav_masks, texts, text_masks)

    loss = criterion(wav_embed, text_embed, num_replicas = 1)
    train_loss.append(loss.item())
    loss /= accu_step
    loss.backward()
    step += 1
    if step % accu_step == 0:
        optimizer.step()
        optimizer.zero_grad()
    '''
    if step == 1500:
        for name, param in model.hubert_layers.hubert.named_parameters():
            if f"layers" in name:
                    param.requires_grad = True
    '''
    acc = 0
    train_accs.append(acc)
    if(step % (config["logging_step"] // config["train_batch_size"]) == 0):
        print(f"Loss: {sum(train_loss) / len(train_loss)}")
  train_loss = sum(train_loss) / len(train_loss)
  train_acc = sum(train_accs) / len(train_accs)

  print(f"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}")

  model.eval()
  valid_loss = []
  valid_accs = []

  for batch in tqdm(valid_loader, file=sys.stdout):
    wavs, wav_masks, texts, text_masks = batch
    wavs = torch.squeeze(wavs, 1).to(device)
    wav_masks = torch.squeeze(wav_masks, 1).to(device)
    texts = torch.squeeze(texts, 1).to(device)
    text_masks = torch.squeeze(text_masks, 1).to(device)
    with torch.no_grad():
        wav_embed, text_embed = model(wavs, wav_masks, texts, text_masks)
    loss = criterion(wav_embed, text_embed, num_replicas = 1)
    acc = 0
    valid_loss.append(loss.item())
    valid_accs.append(acc)
  valid_loss = sum(valid_loss) / len(valid_loss)
  valid_acc = sum(valid_accs) / len(valid_accs)
  if valid_loss >= best_acc:
      best_acc = valid_loss
      print(f"Save model with acc {best_acc}")
      torch.save({"model":model.state_dict(),"optimizer": optimizer.state_dict()}, "st5.ckpt")

  print(f"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}")
